# -*- coding: utf-8 -*-
"""context-reduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZSfPLXcyUUoTP-PJOVEoM_nDBY492ZSo
"""

from underthesea import sent_tokenize
sent_tokenize("Một con vịt con. Hai con vịt, ba con vịt? Bốn con!")

import pandas as pd
df = pd.read_csv("vihallu-private-test-clean.csv")
df

tk_cols = ["context", "prompt", "response"]
df[tk_cols] = df[tk_cols].apply(lambda col: col.map(sent_tokenize))
df

from vllm import LLM
model = LLM(model="Qwen/Qwen3-Embedding-0.6B", task="embed", dtype="float16")

import torch

CLUE_SIM_THRESHOLD = 0.4

def filter_context(example):
    clues, prompts, claims = example["context"], example["prompt"], example["response"]
    input_texts = prompts + claims + clues

    outputs = model.embed(input_texts, use_tqdm=False)
    embeddings = torch.tensor([o.outputs.embedding for o in outputs])

    n_qa = len(prompts) + len(claims)
    similarities = embeddings[:n_qa] @ embeddings[n_qa:].T
    similarities = similarities.max(dim=0).values

    clue_mask = (similarities > CLUE_SIM_THRESHOLD).tolist()
    filtered_clues = [clue for clue, mask in zip(clues, clue_mask) if mask]

    final_context = " ".join(filtered_clues)
    final_prompt = " ".join(prompts)
    final_response = " ".join(claims)
    return {"context": final_context, "prompt": final_prompt, "response": final_response}

filter_context(df.sample(n=1).iloc[0])

from datasets import Dataset
dataset = Dataset.from_pandas(df)
dataset

reduced_dataset_0 = dataset.select(range(1000)).map(filter_context, batched=False)

reduced_dataset_1 = dataset.select(range(1000, 2000)).map(filter_context)

from datasets import concatenate_datasets
reduced_dataset = concatenate_datasets([reduced_dataset_0, reduced_dataset_1])
reduced_dataset

reduced_df = reduced_dataset.to_pandas()
reduced_df.to_csv("vihallu-pvtest-clean-reduced.csv", index=False)
reduced_df